<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>k近邻算法 | Blog of Wangmeng</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一 . K-近邻算法（KNN）概述  最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。   KNN是通过测量不同特征值之间的距离进行分类。它的">
<meta property="og:type" content="article">
<meta property="og:title" content="k近邻算法">
<meta property="og:url" content="http://yoursite.com/2017/09/16/k近邻算法/index.html">
<meta property="og:site_name" content="Blog of Wangmeng">
<meta property="og:description" content="一 . K-近邻算法（KNN）概述  最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。   KNN是通过测量不同特征值之间的距离进行分类。它的">
<meta property="og:updated_time" content="2017-09-16T07:48:26.033Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="k近邻算法">
<meta name="twitter:description" content="一 . K-近邻算法（KNN）概述  最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。   KNN是通过测量不同特征值之间的距离进行分类。它的">
  
    <link rel="alternate" href="/atom.xml" title="Blog of Wangmeng" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Blog of Wangmeng</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-k近邻算法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/16/k近邻算法/" class="article-date">
  <time datetime="2017-09-16T07:47:01.000Z" itemprop="datePublished">2017-09-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      k近邻算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一 . K-近邻算法（KNN）概述 </p>
<pre><code>最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。

 KNN是通过测量不同特征值之间的距离进行分类。它的的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

 下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。
</code></pre><p>由此也说明了KNN算法的结果很大程度取决于K的选择。</p>
<pre><code>在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：
</code></pre><p>同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。</p>
<p>   接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：</p>
<p>1）计算测试数据与各个训练数据之间的距离；</p>
<p>2）按照距离的递增关系进行排序；</p>
<p>3）选取距离最小的K个点；</p>
<p>4）确定前K个点所在类别的出现频率；</p>
<p>5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。</p>
<p>二 .python实现</p>
<p>首先呢，需要说明的是我用的是python3.4.3，里面有一些用法与2.7还是有些出入。</p>
<p>建立一个KNN.py文件对算法的可行性进行验证，如下：</p>
<p>复制代码</p>
<p>#coding:utf-8</p>
<p>from numpy import *<br>import operator</p>
<p>##给出训练数据以及对应的类别<br>def createDataSet():<br>    group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]])<br>    labels = [‘A’,’A’,’B’,’B’]<br>    return group,labels</p>
<p>###通过KNN进行分类<br>def classify(input,dataSe t,label,k):<br>    dataSize = dataSet.shape[0]</p>
<pre><code>####计算欧式距离
diff = tile(input,(dataSize,1)) - dataSet
sqdiff = diff ** 2
squareDist = sum(sqdiff,axis = 1)###行向量分别相加，从而得到新的一个行向量
dist = squareDist ** 0.5

##对距离进行排序
sortedDistIndex = argsort(dist)##argsort()根据元素的值从大到小对元素进行排序，返回下标

classCount={}
for i in range(k):
    voteLabel = label[sortedDistIndex[i]]
    ###对选取的K个样本所属的类别个数进行统计
    classCount[voteLabel] = classCount.get(voteLabel,0) + 1
###选取出现的类别次数最多的类别
maxCount = 0
for key,value in classCount.items():
    if value &gt; maxCount:
        maxCount = value
        classes = key

return classes    
</code></pre><p>复制代码<br>接下来，在命令行窗口输入如下代码：</p>
<p>复制代码</p>
<p>#-<em>-coding:utf-8 -</em>-<br>import sys<br>sys.path.append(“…文件路径…”)<br>import KNN<br>from numpy import *<br>dataSet,labels = KNN.createDataSet()<br>input = array([1.1,0.3])<br>K = 3<br>output = KNN.classify(input,dataSet,labels,K)<br>print(“测试数据为:”,input,”分类结果为：”,output)<br>复制代码<br>回车之后的结果为：</p>
<p>测试数据为： [ 1.1  0.3] 分类为： A</p>
<p>答案符合我们的预期，要证明算法的准确性，势必还需要通过处理复杂问题进行验证，之后另行说明。</p>
<p>这是第一次用python编的一个小程序，势必会遇到各种问题，在此次编程调试过程中遇到了如下问题：</p>
<p>　1 导入.py文件路径有问题，因此需要在最开始加如下代码：</p>
<p>　　import sys<br>　　sys.path.append(“文件路径”)，这样就不会存在路径有误的问题了；</p>
<p>   2 在python提示代码存在问题时，一定要及时改正，改正之后保存之后再执行命令行，这一点跟MATLAB是不一样的，所以在python中最好是敲代码的同时在命令行中一段一段的验证；</p>
<p>　3 在调用文件时函数名一定要写正确，否则会出现：’module’ object has no attribute ‘creatDataSet’；</p>
<p>　4 ‘int’ object has no attribute ‘kclassify’，这个问题出现的原因是之前我讲文件保存名为k.py,在执行</p>
<p>output = K.classify(input,dataSet,labels,K)这一句就会出错。根据函数式编程的思想，每个函数都可以看为是一个变量而将K赋值后，调用k.py时就会出现问题。</p>
<p>三 MATLAB实现<br>   之前一直在用MATLAB做聚类算法的一些优化，其次就是数模的一些常用算法，对于别的算法，还真是没有上手编过，基础还在，思想还在，当然要动手编一下，也是不希望在学python的同时对MATLAB逐渐陌生吧，走走停停，停很重要。<br>   首先，建立KNN.m文件，如下：<br>复制代码<br>%% KNN<br>clear all<br>clc<br>%% data<br>trainData = [1.0,2.0;1.2,0.1;0.1,1.4;0.3,3.5];<br>trainClass = [1,1,2,2];<br>testData = [0.5,2.3];<br>k = 3;</p>
<p>%% distance<br>row = size(trainData,1);<br>col = size(trainData,2);<br>test = repmat(testData,row,1);<br>dis = zeros(1,row);<br>for i = 1:row<br>    diff = 0;<br>    for j = 1:col<br>        diff = diff + (test(i,j) - trainData(i,j)).^2;<br>    end<br>    dis(1,i) = diff.^0.5;<br>end</p>
<p>%% sort<br>jointDis = [dis;trainClass];<br>sortDis= sortrows(jointDis’);<br>sortDisClass = sortDis’;</p>
<p>%% find<br>class = sort(2:1:k);<br>member = unique(class);<br>num = size(member);</p>
<p>max = 0;<br>for i = 1:num<br>    count = find(class == member(i));<br>    if count &gt; max<br>        max = count;<br>        label = member(i);<br>    end<br>end</p>
<p>disp(‘最终的分类结果为：’);<br>fprintf(‘%d\n’,label)<br>复制代码<br>运行之后的结果是，最终的分类结果为：2。和预期结果一样。</p>
<p>总而言之，用MATLAB的时间相对长点，自然也就得心应手点，不过还是希望早点能将python运用自如吧！</p>
<p>三 实战</p>
<pre><code>之前，对KNN进行了一个简单的验证，今天我们使用KNN改进约会网站的效果，个人理解，这个问题也可以转化为其它的比如各个网站迎合客户的喜好所作出的推荐之类的，当然，今天的这个例子功能也实在有限。



在这里根据一个人收集的约会数据，根据主要的样本特征以及得到的分类，对一些未知类别的数据进行分类，大致就是这样。



我使用的是python 3.4.3,首先建立一个文件，例如date.py,具体的代码如下：
</code></pre><p>复制代码</p>
<p>#coding:utf-8</p>
<p>from numpy import *<br>import operator<br>from collections import Counter<br>import matplotlib<br>import matplotlib.pyplot as plt</p>
<p>###导入特征数据<br>def file2matrix(filename):<br>    fr = open(filename)<br>    contain = fr.readlines()###读取文件的所有内容<br>    count = len(contain)<br>    returnMat = zeros((count,3))<br>    classLabelVector = []<br>    index = 0<br>    for line in contain:<br>        line = line.strip() ###截取所有的回车字符<br>        listFromLine = line.split(‘\t’)<br>        returnMat[index,:] = listFromLine[0:3]###选取前三个元素，存储在特征矩阵中<br>        classLabelVector.append(listFromLine[-1])###将列表的最后一列存储到向量classLabelVector中<br>        index += 1</p>
<pre><code>##将列表的最后一列由字符串转化为数字，便于以后的计算
dictClassLabel = Counter(classLabelVector)
classLabel = []
kind = list(dictClassLabel)
for item in classLabelVector:
    if item == kind[0]:
        item = 1
    elif item == kind[1]:
        item = 2
    else:
        item = 3
    classLabel.append(item)
return returnMat,classLabel#####将文本中的数据导入到列表
</code></pre><p>##绘图（可以直观的表示出各特征对分类结果的影响程度）<br>datingDataMat,datingLabels = file2matrix(‘D:\python\Mechine learing in Action\KNN\datingTestSet.txt’)<br>fig = plt.figure()<br>ax = fig.add_subplot(111)<br>ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0<em>array(datingLabels),15.0</em>array(datingLabels))<br>plt.show()</p>
<h2 id="归一化数据-保证特征等权重"><a href="#归一化数据-保证特征等权重" class="headerlink" title="归一化数据,保证特征等权重"></a>归一化数据,保证特征等权重</h2><p>def autoNorm(dataSet):<br>    minVals = dataSet.min(0)<br>    maxVals = dataSet.max(0)<br>    ranges = maxVals - minVals<br>    normDataSet = zeros(shape(dataSet))##建立与dataSet结构一样的矩阵<br>    m = dataSet.shape[0]<br>    for i in range(1,m):<br>        normDataSet[i,:] = (dataSet[i,:] - minVals) / ranges<br>    return normDataSet,ranges,minVals</p>
<p>##KNN算法<br>def classify(input,dataSet,label,k):<br>    dataSize = dataSet.shape[0]</p>
<pre><code>####计算欧式距离
diff = tile(input,(dataSize,1)) - dataSet
sqdiff = diff ** 2
squareDist = sum(sqdiff,axis = 1)###行向量分别相加，从而得到新的一个行向量
dist = squareDist ** 0.5

##对距离进行排序
sortedDistIndex = argsort(dist)##argsort()根据元素的值从大到小对元素进行排序，返回下标

classCount={}
for i in range(k):
    voteLabel = label[sortedDistIndex[i]]
    ###对选取的K个样本所属的类别个数进行统计
    classCount[voteLabel] = classCount.get(voteLabel,0) + 1
###选取出现的类别次数最多的类别
maxCount = 0
for key,value in classCount.items():
    if value &gt; maxCount:
        maxCount = value
        classes = key
return classes
</code></pre><p>##测试(选取10%测试）<br>def datingTest():<br>    rate = 0.10<br>    datingDataMat,datingLabels = file2matrix(‘D:\python\Mechine learing in Action\KNN\datingTestSet.txt’)<br>    normMat,ranges,minVals = autoNorm(datingDataMat)<br>    m = normMat.shape[0]<br>    testNum = int(m * rate)<br>    errorCount = 0.0<br>    for i in range(1,testNum):<br>        classifyResult = classify(normMat[i,:],normMat[testNum:m,:],datingLabels[testNum:m],3)<br>        print(“分类后的结果为:,”, classifyResult)<br>        print(“原结果为：”,datingLabels[i])<br>        if(classifyResult != datingLabels[i]):<br>                                  errorCount += 1.0<br>    print(“误分率为:”,(errorCount/float(testNum)))</p>
<p>###预测函数<br>def classifyPerson():<br>    resultList = [‘一点也不喜欢’,’有一丢丢喜欢’,’灰常喜欢’]<br>    percentTats = float(input(“玩视频所占的时间比?”))<br>    miles = float(input(“每年获得的飞行常客里程数?”))<br>    iceCream = float(input(“每周所消费的冰淇淋公升数?”))<br>    datingDataMat,datingLabels = file2matrix(‘D:\python\Mechine learing in Action\KNN\datingTestSet2.txt’)<br>    normMat,ranges,minVals = autoNorm(datingDataMat)<br>    inArr = array([miles,percentTats,iceCream])<br>    classifierResult = classify((inArr-minVals)/ranges,normMat,datingLabels,3)<br>    print(“你对这个人的喜欢程度:”,resultList[classifierResult - 1])<br>复制代码</p>
<p>新建test.py文件了解程序的运行结果，代码：</p>
<p>复制代码</p>
<p>#coding:utf-8</p>
<p>from numpy import *<br>import operator<br>from collections import Counter<br>import matplotlib<br>import matplotlib.pyplot as plt</p>
<p>import sys<br>sys.path.append(“D:\python\Mechine learing in Action\KNN”)<br>import date<br>date.classifyPerson()<br>复制代码</p>
<p>运行结果如下图：</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/09/16/k近邻算法/" data-id="cj7n0habm00011wjhw84ouxiw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/05/28/c++ in vscode/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">c++ with vscode</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/09/16/k近邻算法/">k近邻算法</a>
          </li>
        
          <li>
            <a href="/2017/05/28/c++ in vscode/">c++ with vscode</a>
          </li>
        
          <li>
            <a href="/2017/05/13/test-article/">test article</a>
          </li>
        
          <li>
            <a href="/2017/05/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 roguedream<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>